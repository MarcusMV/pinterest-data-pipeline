{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount S3 and read objects from topics to spark df\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"user-12c5e9eb47cb-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/mount_name\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)\n",
    "\n",
    "# Unmount drive if necessary\n",
    "# dbutils.fs.unmount(\"/mnt/mount_name\")\n",
    "\n",
    "def read_from_s3(topic):\n",
    "    file_location = f\"/mnt/mount_name/topics/12c5e9eb47cb.{topic}/partition=0/*.json\" \n",
    "    file_type = \"json\"\n",
    "    # Ask Spark to infer the schema\n",
    "    infer_schema = \"true\"\n",
    "    # Read in JSONs from mounted S3 bucket\n",
    "    df = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .load(file_location)\n",
    "    # Display Spark dataframe to check its content\n",
    "    return df\n",
    "\n",
    "topics = {'pin', 'geo', 'user'}\n",
    "dfs_container = {f'df_{key}': read_from_s3(key) for key in topics}\n",
    "\n",
    "# display(dfs_container['df_pin'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
