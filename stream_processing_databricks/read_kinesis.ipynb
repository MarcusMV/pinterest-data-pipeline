{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQuery\n",
    "\n",
    "# First read credentials same as in ./batch_processing_databricks/read_creds.py\n",
    "\n",
    "def read_from_kinesis(topic):\n",
    "    df = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', f'streaming-12c5e9eb47cb-{topic}') \\\n",
    "    .option('initialPosition','earliest') \\\n",
    "    .option('region','us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "\n",
    "    return df\n",
    "\n",
    "def write_to_delta_table(df: DataFrame, table_name: str) -> StreamingQuery:\n",
    "    # Delete checkpoint location first\n",
    "    dbutils.fs.rm(f\"/tmp/kinesis/{table_name}_checkpoints/\", True)\n",
    "\n",
    "    query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", f\"/tmp/kinesis/{table_name}_checkpoints/\") \\\n",
    "    .table(table_name)\n",
    "\n",
    "    print(f\"Writing to delta table {table_name}..\")\n",
    "    return query"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
